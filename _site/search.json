[
  {
    "objectID": "posts/blog_1/index.html",
    "href": "posts/blog_1/index.html",
    "title": "Can the tSNE results be reproducible?",
    "section": "",
    "text": "If you are an R user struggling to access HARPS GTO sample dataset and have the question that Can‚Äôt we reproduce the t-distributed stochastic neighbor embedding (tSNE) results that already exist? In their mind, this blog is for you. Today, I will introduce my workflow of accessing processed data from original data, downloading the processed data, and data processing to initialize cluster labels and show my surprising findings on reproducing tSNE results.\nThis example is relevant to the paper, ‚ÄúDissecting stellar chemical abundance space with t-SNE‚Äù (Anders et al.¬†2018, Astronomy & Astrophysics 619, A125).\nFor simplicity, I added all the codes separately and linked them through the process.\nPlease note that you will need to \\(\\color{green}{\\text{install python library}}\\) called open_data, which you can find in open_data.py as the initial step."
  },
  {
    "objectID": "posts/blog_1/index.html#data-processing-steps-already-followed-by-open_data-library",
    "href": "posts/blog_1/index.html#data-processing-steps-already-followed-by-open_data-library",
    "title": "Can the tSNE results be reproducible?",
    "section": "Data processing steps already followed by open_data library",
    "text": "Data processing steps already followed by open_data library\nThe original data can be found in the GitHub repository of the authors as DelgadoMena2017.fits, which includes some existing cluster labels and abundance determination for \\(\\color{blue}{\\text{Mg, Al, Si, Ca, TiI, Fe, Cu, Zn, Sr, Y, ZrII, Ce, and Ba}}\\). This file contains details regarding \\(\\color{red}{\\text{1059}}\\) stars.\nThen, how do we get only \\(\\color{red}{\\text{530}}\\) stars? ü§î\nLet‚Äôs investigate‚Ä¶\nAccording to the authors, the sample needed to be analyzed in a more restricted temperature range to obtain reliable tSNE abundance maps. The reason is that specific abundance trends dominate underlying temperature trends. Therefore,\n\nchoose an effective temperature range of \\(\\color{brown}{5300 \\text{ K} < T_{eff} < 6000 \\text{ K}}\\).\n\nOnly \\(\\color{green}{539}\\) stars were satisfied in this step. Next,\n\nexclude stars with \\(\\color{brown}{\\text{log } g_{HIP}} < 3\\) which remove \\(\\color{green}{\\text{one}}\\) star.\n\nThen,\n\nselect successful abundance determination for \\(\\color{brown}{\\text{Mg, Al, Si, Ca, TiI, Fe, Cu, Zn, Sr, Y, ZrII, Ce, and Ba}}\\) which use as input for tSNE.\n\nOnly \\(\\color{green}{533}\\) stars have remained because others contain missing values. Furthermore,\n\nto compensate for the fact that tSNE does not take into account individual (heteroscedastic) uncertainties in the data, the authors followed the approach of Hogg et al.¬†(2016) and rescaled each abundance by the median uncertainty in that element, assuming an abundance uncertainty floor of 0.03 dex. Additionally,\nremove stars that did not converge by using the age determination code, StarHose code.\n\nThe \\(\\color{green}{3}\\) stars are discarded.\nThere are only \\(\\color{green}{530}\\) stars in our final sample. üëè\nNote: You do not need to consider the above steps because the open_data package has already done it for us."
  },
  {
    "objectID": "posts/blog_1/index.html#our-workflow",
    "href": "posts/blog_1/index.html#our-workflow",
    "title": "Can the tSNE results be reproducible?",
    "section": "Our workflow",
    "text": "Our workflow\nLet‚Äôs begin‚Ä¶ ü§ì\n\nTo download the processed data as a CSV file and save it locally, you need to run download_original.py,\nRun this code to process data further as indicated in the original paper to compute missing variables, new variables, cluster labels and produce plots of authors t-SNE results,\nRun this code to conduct our own t-SNE results.\n\nHere, I used the \\(\\color{red}{\\text{Perplexity parameter as 40}}\\) authors already mentioned that in the paper.\nHere is the output..ü§ó"
  },
  {
    "objectID": "posts/blog_1/index.html#acknowledge",
    "href": "posts/blog_1/index.html#acknowledge",
    "title": "Can the tSNE results be reproducible?",
    "section": "Acknowledge",
    "text": "Acknowledge\nI am grateful for Prof.¬†Dianne Cook who suggested me as an exercise for me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Team",
    "section": "",
    "text": "I am a first year PhD student in Department of Econometrics and Business Statistics at the Monash University, Australia. This website contains the findings of my PhD research project.\nMy supervisors are:\n\nProfessor Dianne Cook\nDr.¬†Michael Lydeamore\nDr.¬†Paul Harrison\nDr.¬†Thiyanga S. Talagala"
  },
  {
    "objectID": "Blogs.html",
    "href": "Blogs.html",
    "title": "Posts",
    "section": "",
    "text": "Is neighborhood parameter really matter in non-linear dimensionality reduction (NLDR) techniques?\n\n\n\n\n\n\n\nNLDR\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\n  \n\n\n\n\nLow-dimensional representation\n\n\n\n\n\n\n\nBasics\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\n  \n\n\n\n\nHow to find an effective value for the number of bins partitioning the range of x-axis?\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs there any difference between dimension reductions in \\(2-d\\) and \\(3-d\\)?\n\n\n\n\n\n\n\ntSNE\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow do we set up a Python virtual environment in the R shinyapps.io server?\n\n\n\n\n\n\n\nTechnichal\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\n  \n\n\n\n\nCan the tSNE results be reproducible?\n\n\n\n\n\n\n\ntSNE\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog_2/index.html",
    "href": "posts/blog_2/index.html",
    "title": "How do we set up a Python virtual environment in the R shinyapps.io server?",
    "section": "",
    "text": "Today, I will introduce what are the initial steps that you need to follow when setting up a Python virtual environment in R for the shinyapps.io server.\nNote: You need to have an account in shinyapps.io server. The page can be found in here.\nFirst, you need to create a ‚Äò.Rprofile‚Äô file within the directory of your shiny app. Then, initialize a suitable name for the \\(\\color{red}{\\text{Python virtual environment}}\\).\n\nVIRTUALENV_NAME = \"new_env\"\n\nAfter that, we need to set environment variables in the ‚Äò.Rprofile‚Äô file as follows.\n\nSys.setenv(PYTHON_PATH = 'python3')\n# Installs into default shiny virtualenvs dir\nSys.setenv(VIRTUALENV_NAME = VIRTUALENV_NAME) \nSys.setenv(RETICULATE_PYTHON = paste0('/home/shiny/.virtualenvs/', \n                                      VIRTUALENV_NAME, '/bin/python'))\n\nThe next step is to create the \\(\\color{red}{\\text{Python virtual environment}}\\). To do that, you can write the following code chunks in server.R or your shiny app script.\nIn there, first of all, you need to get the environment variables.\n\nvirtualenv_dir = Sys.getenv(\"VIRTUALENV_NAME\")\npython_path = Sys.getenv(\"PYTHON_PATH\")\n\nNext, create a Python virtual environment by specifying the Python path.\n\nreticulate::virtualenv_create(virtualenv_dir, python = python_path)\n\nThen, you have to install Python dependencies. To do this, you can install the packages by directly specifying them.\n\nreticulate::virtualenv_install(virtualenv_dir, packages = c(\"pandas==1.3.5\")) \n\nIf not, can use requirement.txt which contains all the packages.\n\nreticulate::virtualenv_install(virtualenv_dir, c(\"-r\", \"requirements.txt\"))\n\nFinally, define the Python virtual environment to be used by reticulate.\n\nreticulate::use_virtualenv(virtualenv_dir, required = TRUE)\n\nDone‚Ä¶üëè"
  },
  {
    "objectID": "Learning.html",
    "href": "Learning.html",
    "title": "Introduction to topics",
    "section": "",
    "text": "Local vs global structure\n\n\n\n\n\n\n\nBasics\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning/learning_1/index.html",
    "href": "learning/learning_1/index.html",
    "title": "Local vs global structure",
    "section": "",
    "text": "library(Rtsne)\nlibrary(umap)\nlibrary(phateR)\n\nWarning: package 'phateR' was built under R version 4.2.2\n\n\nLoading required package: Matrix\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(langevitour)\nlibrary(reticulate)\nlibrary(ggplot2)\nuse_python(\"~/miniforge3/envs/pcamp_env/bin/python\")\nuse_condaenv(\"pcamp_env\")"
  },
  {
    "objectID": "learning/learning_1/index.html#references",
    "href": "learning/learning_1/index.html#references",
    "title": "Local vs global structure",
    "section": "References",
    "text": "References\nhttps://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"
  },
  {
    "objectID": "posts/blog_3/index.html",
    "href": "posts/blog_3/index.html",
    "title": "Low-dimensional representation",
    "section": "",
    "text": "Today, I will introduce some basic terminologies regarding low-dimensional representation and types of dimensionality reduction techniques and visualize some outcomes of dimensionality reductions of an example dataset.\nLet‚Äôs begin‚Ä¶ü§ì\nThe first question that comes to your mind is,"
  },
  {
    "objectID": "learning/learning_2/index.html",
    "href": "learning/learning_2/index.html",
    "title": "What is meant by low-dimensional representation?",
    "section": "",
    "text": "Today, I will introduce some basic terminologies regarding low-dimensional representation and types of low-dimensional reduction techniques and visualize some outcomes of low-dimensional reductions of an example dataset.\nLet‚Äôs begin‚Ä¶\nThe first question that comes to your mind is, ‚ÄúWhat is meant by low-dimensional representation?‚Äù.\nA low-dimensional representation is a visualization of the dimension reductions on high-dimensional data.\nGot it‚Ä¶\nI know what the next problem you have now is. How do we define a data set as a high-dimensional data set?\nSuppose the number of features (variable observed) is close to or larger than the number of observations (or data points). In that case, the data set is specified as high-dimensional. On the other hand, a low-dimensional data set in which the number of features is less than the number of observations.\nNext,\nWhy do we need a low-dimensional representation?\nThere are various applications in the field of machine learning and deep learning. For example, a low-dimensional data representation can be used as a noise-removal or feature-extraction technique.\nAre there any challenges with high-dimensional data?\nOne of our challenges is that analyzing high-dimensional data requires considering potential problems with more features than observations.\nIn addition, most classical statistical methods are set up for low-dimensional data because low-dimensional data were much more common in the past when data collection was more difficult and time-consuming. However, the development of information technology has allowed large amounts of data to be collected and stored with relative ease, allowing large numbers of features to be collected in recent years.\nAlso, visualizing a large number of features takes a lot of work.\nI hope you all have a basic idea about the terminologies and the challenges with high-dimensional data. The necessity of low-dimension reduction techniques comes as a remedy for the above challenges, especially in the visualization task.\nDimensionality reduction is a common step for data processing. This process is helpful for feature engineering or data visualization. From here onwards, I focus on how the visualization looks in various dimension reduction techniques.\nBefore starting, how many dimensions are defined as low-dimensional reductions?\nGenerally, most dimension reduction techniques allow the reduction of high-dimensional data to two or three dimensions (eg: t-distributed stochastic neighbor embedding (t-SNE)).\nDimension reduction tools for visualization can be categorized as follows.\nLinear dimension reduction techniques\n\nPrincipal Component Analysis (PCA)\nLinear Discriminant Analysis (LDA)\nMultidimensional Scaling (MDS)\n\nNon-linear dimension reduction techniques (Manifold learning)\n\nt-distributed stochastic neighbor embedding (tSNE)\nUniform Manifold Approximation and Projection (UMAP)\nPotential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)\nLarge-scale Dimensionality Reduction Using Triplets (TriMAP)\nPairwise Controlled Manifold Approximation (PaCMAP)\nAuto Encoder (AE)\n\nThe high variance directions reflect the key trends in the data set in a linear dimensionality reduction technique. For example, a new axis is linear and may be non-orthogonal [@paper2]. In contrast, non-linear dimensionality reduction utilizes non-linear kernels to locate key data set trends while preserving the original data‚Äôs local and global structure (neighborhood relation). In terms of preserving the local and global structure, the distance or distance ranks between data points in the original space should be preserved after reducing the dimensions to low-dimensional space.\nYou can find more details about the local and global structure here.\nExample\nA simulated dataset is generated to obtain a \\(2-d\\) plane in \\(4-d\\) with little noise in third and fourth dimensions.\n\n\n\n\n\nCode\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\nlangevitour(df_2)\n\n\n\n\n\nFigure¬†1: Visualization of the simulated dataset to obtain a \\(2-d\\) plane in \\(4-d\\).\n\n\n\nLet‚Äôs visualize PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP and see whether they preserve the \\(2-d\\) plane‚Äôs structure after applying dimension reductions.\nNote: The default parameters are used to perform dimensionality reduction techniques.\n\n\nCode\n## PCA\n\ncalculate_pca <- function(feature_dataset, num_pcs){\n  pcaY_cal <- prcomp(feature_dataset, center = TRUE, scale = TRUE)\n  PCAresults <- data.frame(pcaY_cal$x[, 1:4])\n  summary_pca <- summary(pcaY_cal)\n  var_explained_df <- data.frame(PC= paste0(\"PC\",1:4),\n                               var_explained=(pcaY_cal$sdev[1:4])^2/sum((pcaY_cal$sdev[1:4])^2))\n  return(list(prcomp_out = pcaY_cal,pca_components = PCAresults, summary = summary_pca, var_explained_pca  = var_explained_df))\n}\n\npca_ref_calc <- calculate_pca(df_2,4) \nvar_explained_df <- pca_ref_calc$var_explained_pca\nPCA_df <- pca_ref_calc$pca_components\n\nPCA_df_plot1 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC2)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot2 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot3 <- PCA_df %>%\n  ggplot(aes(x = PC2,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\n## tSNE\nset.seed(100001)\ntSNE_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  Rtsne()\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()  \n\nnames(tSNE_df)[1:2] <- c(\"tSNE1\", \"tSNE2\")\n\ntSNE_df_plot <- tSNE_df %>%\n  ggplot(aes(x = tSNE1,\n             y = tSNE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## UMAP\nset.seed(100002)\nUMAP_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  umap()\n\nUMAP_df <- UMAP_fit$layout %>%\n  as.data.frame()  \n\nnames(UMAP_df)[1:2] <- c(\"UMAP1\", \"UMAP2\")\n\nUMAP_df_plot <- UMAP_df %>%\n  ggplot(aes(x = UMAP1,\n             y = UMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## Phate\n\n# use_python(\"~/miniforge3/envs/pcamp_env/bin/python\")\n# use_condaenv(\"pcamp_env\")\n\n# set.seed(100003)\n# tree_phate_fit <- phate(df_2, 2)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding) %>%\n#   mutate(ID=row_number())\n# \n# names(PHATE_df)[1:2] <- c(\"PHATE1\", \"PHATE2\")\n# write.csv(PHATE_df, paste0(here::here(), \"/learning/learning_2/PHATE_df.csv\"))\n\nPHATE_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PHATE_df.csv\"))\n\nPHATE_df_plot <- PHATE_df %>%\n  ggplot(aes(x = PHATE1,\n             y = PHATE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## TriMAP\n\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# set.seed(100004)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# write.csv(df_TriMAP, paste0(here::here(), \"/learning/learning_2/TriMAP_df.csv\"))\n\n  \n\nTriMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/TriMAP_df.csv\"))\n\nTriMAP_df_plot <- TriMAP_df %>%\n  ggplot(aes(x = TriMAP1,\n             y = TriMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## PaCMAP\n\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# set.seed(100005)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# \n# write.csv(df_PaCMAP, paste0(here::here(), \"/learning/learning_2/PaCMAP_df.csv\"))\n\n\nPaCMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PaCMAP_df.csv\"))\n\nPaCMAP_df_plot <- PaCMAP_df %>%\n  ggplot(aes(x = PaCMAP1,\n             y = PaCMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n (PCA_df_plot1 + PCA_df_plot2 + PCA_df_plot3 ) / ( tSNE_df_plot + UMAP_df_plot + PHATE_df_plot )/( TriMAP_df_plot + PaCMAP_df_plot)\n\n\n\n\n\nFigure¬†2: Visualization by PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP\n\n\n\n\nAs shown in Figure Figure¬†2, you can see the outcomes visualized by some dimensionality reduction techniques.\nIn our next blog, we will see why these dimensionality reduction representations differ from the original structure in high-dimensional space."
  },
  {
    "objectID": "posts/blog_4/index.html",
    "href": "posts/blog_4/index.html",
    "title": "Is there any difference between dimension reductions in \\(2-d\\) and \\(3-d\\)?",
    "section": "",
    "text": "Generally, we reduce the dimensions to 2-d and see the visualization. But you also can try to reduce to \\(3-d\\) as well.\nThe main issue you would in mind is that can it preserve the same structure as in \\(2-d\\).\nLet‚Äôs look an example‚Ä¶\n\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\n\nN <- dim(df_2)[1]\nopt_perplexity <- sqrt(N)\n\ntSNE_fit <- df_2 %>%\n    select(where(is.numeric)) %>%\n    Rtsne(perplexity = opt_perplexity, pca = FALSE, pca_center = FALSE, normalize = FALSE, dims = 2)\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()  %>%\n  mutate(ID=row_number())\n\nnames(tSNE_df)[1:(ncol(tSNE_df)-1)] <- paste0(rep(\"tSNE\",(ncol(tSNE_df)-1)), 1:(ncol(tSNE_df)-1))\n\ntSNE_df_plot <- tSNE_df %>%\n    ggplot(aes(x = tSNE1,\n               y = tSNE2))+\n    geom_point() +\n    coord_equal()\n\ntSNE_df_plot\n\n\n\n\n\ntSNE_fit1 <- df_2 %>%\n    select(where(is.numeric)) %>%\n    Rtsne(perplexity = opt_perplexity, pca = FALSE, pca_center = FALSE, normalize = FALSE, dims = 3)\n\ntSNE_df1 <- tSNE_fit1$Y %>%\n  as.data.frame()  %>%\n  mutate(ID=row_number())\n\nnames(tSNE_df1)[1:(ncol(tSNE_df1)-1)] <- paste0(rep(\"tSNE\",(ncol(tSNE_df1)-1)), 1:(ncol(tSNE_df1)-1))\n\nlangevitour(tSNE_df1 %>% select(-ID))"
  },
  {
    "objectID": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation.",
    "href": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation.",
    "title": "What is meant by low-dimensional representation?",
    "section": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\).",
    "text": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\).\nA low-dimensional representation is a visualization of the dimension reductions on high-dimensional data.\nGot it‚Ä¶üëè\nI know what the next problem you have now is."
  },
  {
    "objectID": "posts/blog_3/index.html#how-do-we-define-a-data-set-as-a-high-dimensional-data-set",
    "href": "posts/blog_3/index.html#how-do-we-define-a-data-set-as-a-high-dimensional-data-set",
    "title": "Low-dimensional representation",
    "section": "How do we define a data set as a high-dimensional data set?",
    "text": "How do we define a data set as a high-dimensional data set?\nSuppose the number of features (variable observed) is close to or larger than the number of observations (or data points). In that case, the data set is specified as \\(\\color{green}{\\text{high-dimensional}}\\). On the other hand, a low-dimensional data set in which the number of features is less than the number of observations.\nNext,"
  },
  {
    "objectID": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation",
    "href": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation",
    "title": "Low-dimensional representation",
    "section": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\)",
    "text": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\)\nA low-dimensional representation is a visualization of the dimension reductions on high-dimensional data.\nGot it‚Ä¶üëè\nI know what the next problem you have now is."
  },
  {
    "objectID": "posts/blog_3/index.html#why-do-we-need-a-low-dimensional-representation",
    "href": "posts/blog_3/index.html#why-do-we-need-a-low-dimensional-representation",
    "title": "Low-dimensional representation",
    "section": "Why do we need a low-dimensional representation?",
    "text": "Why do we need a low-dimensional representation?\nThere are various applications in the field of machine learning and deep learning. For example, a low-dimensional data representation can be used as a noise-removal or feature-extraction technique."
  },
  {
    "objectID": "posts/blog_3/index.html#are-there-any-challenges-with-high-dimensional-data",
    "href": "posts/blog_3/index.html#are-there-any-challenges-with-high-dimensional-data",
    "title": "Low-dimensional representation",
    "section": "Are there any challenges with high-dimensional data?",
    "text": "Are there any challenges with high-dimensional data?\nOne of our challenges is that analyzing high-dimensional data requires considering potential problems with more features than observations.\nIn addition, most classical statistical methods are set up for low-dimensional data because low-dimensional data were much more common in the past when data collection was more difficult and time-consuming. However, the development of information technology has allowed large amounts of data to be collected and stored with relative ease, allowing large numbers of features to be collected in recent years.\nAlso, visualizing a large number of features takes a lot of work.\nI hope you all have a basic idea about the terminologies and the challenges with high-dimensional data. üëè"
  },
  {
    "objectID": "posts/blog_3/index.html#dimensionality-reduction-techniques",
    "href": "posts/blog_3/index.html#dimensionality-reduction-techniques",
    "title": "Low-dimensional representation",
    "section": "Dimensionality reduction techniques",
    "text": "Dimensionality reduction techniques\nThe necessity of dimensionality reduction techniques comes as a remedy for the above challenges, especially in the visualization task.\nDimensionality reduction is a common step for data processing. This process is helpful for feature engineering or data visualization. From here on wards, I focus on how the visualization looks in various dimension reduction techniques.\nBefore starting, \\(\\color{blue}{\\text{how many dimensions are defined as dimensional reductions in low-dimensional space?}}\\)\nGenerally, most dimension reduction techniques allow the reduction of high-dimensional data to two or three dimensions (eg: t-distributed stochastic neighbor embedding (t-SNE)).\nDimension reduction tools for visualization can be categorized as follows.\n\n\n\n\n\n\n\nLinear dimensionality reduction\ntechniques\nNon-linear dimensionality reduction\ntechniques (Manifold learning)\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\n\nt-distributed stochastic neighbor\nembedding (tSNE)\n\n\n\n\nLinear Discriminant Analysis (LDA)\n\n\nUniform Manifold Approximation and\nProjection (UMAP)\n\n\n\n\nMultidimensional Scaling (MDS)\n\n\nPotential of Heat-diffusion for Affinity-based\nTrajectory Embedding (PHATE)\n\n\n\n\nFactorial Analysis (FA)\n\n\nLarge-scale Dimensionality Reduction\nUsing Triplets (TriMAP)\n\n\n\n\nTruncated Singular Value Decomposition (SVD)\n\n\nPairwise Controlled Manifold\nApproximation (PaCMAP)\n\n\n\n\nGeneralized discriminant analysis (GDA)\n\n\nAuto Encoder (AE)\n\n\n\n\nThe high variance directions reflect the key trends in the data set in a linear dimensionality reduction technique. For example, a new axis is linear and may be non-orthogonal (Parashar et al. 2019). In contrast, non-linear dimensionality reduction utilizes non-linear kernels to locate key data set trends while preserving the original data‚Äôs local and global structure (neighborhood relation). In terms of preserving the local and global structure, the distance or distance ranks between data points in the original space should be preserved after reducing the dimensions to low-dimensional space.\nYou can find more details about the local and global structure here.\n\nExample\nA simulated dataset is generated to obtain a \\(2-d\\) plane in \\(4-d\\) with little noise in third and fourth dimensions.\n\n\n\n\n\nCode\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\nlangevitour(df_2)\n\n\n\n\n\nFigure¬†1: Visualization of the simulated dataset to obtain a \\(2-d\\) plane in \\(4-d\\).\n\n\n\nLet‚Äôs visualize PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP and see whether they preserve the \\(2-d\\) plane‚Äôs structure after applying dimension reductions.\nNote: The default parameters are used to perform dimensionality reduction techniques.\n\n\nCode\n## PCA\n\ncalculate_pca <- function(feature_dataset, num_pcs){\n  pcaY_cal <- prcomp(feature_dataset, center = TRUE, scale = TRUE)\n  PCAresults <- data.frame(pcaY_cal$x[, 1:4])\n  summary_pca <- summary(pcaY_cal)\n  var_explained_df <- data.frame(PC= paste0(\"PC\",1:4),\n                               var_explained=(pcaY_cal$sdev[1:4])^2/sum((pcaY_cal$sdev[1:4])^2))\n  return(list(prcomp_out = pcaY_cal,pca_components = PCAresults, summary = summary_pca, var_explained_pca  = var_explained_df))\n}\n\npca_ref_calc <- calculate_pca(df_2,4) \nvar_explained_df <- pca_ref_calc$var_explained_pca\nPCA_df <- pca_ref_calc$pca_components\n\nPCA_df_plot1 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC2)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot2 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot3 <- PCA_df %>%\n  ggplot(aes(x = PC2,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\n## tSNE\nset.seed(100001)\ntSNE_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  Rtsne()\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()  \n\nnames(tSNE_df)[1:2] <- c(\"tSNE1\", \"tSNE2\")\n\ntSNE_df_plot <- tSNE_df %>%\n  ggplot(aes(x = tSNE1,\n             y = tSNE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## UMAP\nset.seed(100002)\nUMAP_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  umap()\n\nUMAP_df <- UMAP_fit$layout %>%\n  as.data.frame()  \n\nnames(UMAP_df)[1:2] <- c(\"UMAP1\", \"UMAP2\")\n\nUMAP_df_plot <- UMAP_df %>%\n  ggplot(aes(x = UMAP1,\n             y = UMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## Phate\n\n# use_python(\"~/miniforge3/envs/pcamp_env/bin/python\")\n# use_condaenv(\"pcamp_env\")\n\n# set.seed(100003)\n# tree_phate_fit <- phate(df_2, 2)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding) %>%\n#   mutate(ID=row_number())\n# \n# names(PHATE_df)[1:2] <- c(\"PHATE1\", \"PHATE2\")\n# write.csv(PHATE_df, paste0(here::here(), \"/learning/learning_2/PHATE_df.csv\"))\n\nPHATE_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PHATE_df.csv\"))\n\nPHATE_df_plot <- PHATE_df %>%\n  ggplot(aes(x = PHATE1,\n             y = PHATE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## TriMAP\n\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# set.seed(100004)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# write.csv(df_TriMAP, paste0(here::here(), \"/learning/learning_2/TriMAP_df.csv\"))\n\n  \n\nTriMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/TriMAP_df.csv\"))\n\nTriMAP_df_plot <- TriMAP_df %>%\n  ggplot(aes(x = TriMAP1,\n             y = TriMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## PaCMAP\n\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# set.seed(100005)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# \n# write.csv(df_PaCMAP, paste0(here::here(), \"/learning/learning_2/PaCMAP_df.csv\"))\n\n\nPaCMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PaCMAP_df.csv\"))\n\nPaCMAP_df_plot <- PaCMAP_df %>%\n  ggplot(aes(x = PaCMAP1,\n             y = PaCMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n (PCA_df_plot1 + PCA_df_plot2 + PCA_df_plot3 ) / ( tSNE_df_plot + UMAP_df_plot + PHATE_df_plot )/( TriMAP_df_plot + PaCMAP_df_plot)\n\n\n\n\n\nFigure¬†2: Visualization by PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP\n\n\n\n\nAs shown in Figure¬†2, you can see the outcomes visualized by some dimensionality reduction techniques.\nIn our next blog, we will see how and why these dimensionality reduction representations differ from the original structure in high-dimensional space."
  },
  {
    "objectID": "posts/blog_6/index.html",
    "href": "posts/blog_6/index.html",
    "title": "Is neighborhood parameter really matter in non-linear dimensionality reduction (NLDR) techniques?",
    "section": "",
    "text": "In this blog post, I will explore neighborhood parameters of t-SNE, UMAP, PHATE, PaCMAP, and TriMAP and compare the results with some simulated data sets.\nFirst of all, let‚Äôs identify what is meant by a neighborhood parameter of an NLDR technique. \\(\\color{red}{\\text{The neighborhood parameter represents the number of close neighbors each point has}}\\). In each NLDR technique, different naming is used as a function parameter. The following table shows more details."
  },
  {
    "objectID": "posts/blog_6/index.html#references",
    "href": "posts/blog_6/index.html#references",
    "title": "Is neighborhood parameter really matter in non-linear dimensionality reduction (NLDR) techniques?",
    "section": "References",
    "text": "References\nhttps://pair-code.github.io/understanding-umap/\nhttps://distill.pub/2016/misread-tsne/"
  },
  {
    "objectID": "posts/blog_5/index.html",
    "href": "posts/blog_5/index.html",
    "title": "How to find an effective value for the number of bins partitioning the range of x-axis?",
    "section": "",
    "text": "How to find an effective value for the number of bins partitioning the range of the x-axis?\nIn this blog, I will explore hexagonal binning and define a workflow to find an effective value for the number of bins along the x-axis. Regarding dimensionality reduction techniques, the x-axis means the first embedding or the first dimensionality reduction component."
  },
  {
    "objectID": "posts/blog_5/index.html#hexagonal-binning",
    "href": "posts/blog_5/index.html#hexagonal-binning",
    "title": "How to find an effective value for the number of bins partitioning the range of x-axis?",
    "section": "Hexagonal binning",
    "text": "Hexagonal binning"
  },
  {
    "objectID": "posts/blog_5/index.html#example",
    "href": "posts/blog_5/index.html#example",
    "title": "How to find an effective value for the number of bins partitioning the range of x-axis?",
    "section": "Example",
    "text": "Example\nA simulated dataset is generated to obtain a \\(2-d\\) plane in \\(4-d\\) with little noise in third and fourth dimensions.\n\n\n\n\n\nCode\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\nlangevitour(df_2)\n\n\n\n\n\nFigure¬†1: Visualization of the simulated dataset to obtain a \\(2-d\\) plane in \\(4-d\\).\n\n\n\n\n\nCode\n# plot_list <- list()\n# \n# # Create the loop.vector \n# loop_vector <- c(5, 10, 30, 40, 50, 80)\n# \n# for(i in c(1:6))\n# { # Loop over loop.vector\n#   num_bins <- loop_vector[i] \n#   plot_list[[i]] <- draw_hex_bin(tSNE_df, num_bins) + ggtitle(paste0(\"No. of bins: \", num_bins))+ \n#     theme(plot.title = element_text(size = 7, hjust = 0.5, vjust = -0.5)) + theme(legend.position = \"none\")\n# }\n# plot_grob <- arrangeGrob(grobs=plot_list, ncol = 3)\n# grid.arrange(plot_grob)"
  },
  {
    "objectID": "posts/blog_6/index.html#the-default-value-is-always-not-good.",
    "href": "posts/blog_6/index.html#the-default-value-is-always-not-good.",
    "title": "Is neighborhood parameter really matter in non-linear dimensionality reduction (NLDR) techniques?",
    "section": "The default value is always not good.",
    "text": "The default value is always not good.\nLet‚Äôs start with a simple example. To make things as simple as possible, I‚Äôll consider two clusters in a 2D plane, as shown in Figure¬†1.\n\n\n\n\n\n\n\n\nCode\nset.seed(202303001)\ndf1 <- tibble::tibble(x1=rnorm(100, mean = 1, sd = 0.05), x2=rnorm(100, mean = 0, sd = 0.05))\ndf1$class <- rep(\"Class 1\",100)\n\ndf2 <- tibble::tibble(x1=rnorm(100, mean = 0, sd = 0.05), x2=rnorm(100, mean = 1, sd = 0.05))\ndf2$class <- rep(\"Class 2\",100)\n\ndf_2 <- rbind(df1, df2)\n\ndf_2_with_label <- df_2 %>%\n    ggplot(aes(x = x1,\n               y = x2, color = df_2$class))+\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1)\n\ndf_2_with_label\n\n\n\n\n\nFigure¬†1: Original data of two clusters in a 2D plane.\n\n\n\n\nFirst, compare the results of t-SNE, UMAP, PHATE, PaCMAP, and TriMAP with the default value of the neighborhood parameter. Figure¬†2 shows the 2D embedding visualization of considered NLDR techniques.\n\n\nCode\ndata_pca <- df_2 %>%\n  select(where(is.numeric))\n\n# ## tSNE\n# \n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# tSNE_fit <- data_pca %>%\n#   Rtsne(perplexity = 30)\n# \n# tSNE_df <- tSNE_fit$Y %>%\n#   as.data.frame() \n# names(tSNE_df) <- c('embedding_1', 'embedding_2')\n# tSNE_df$type <- rep(\"tSNE\", nrow(data_pca))\n# \n# ## UMAP\n# \n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# UMAP_fit <- data_pca %>%\n#   umap(n_neighbors = 15)\n# \n# UMAP_df <- UMAP_fit$layout %>%\n#   as.data.frame()  \n# names(UMAP_df) <- c('embedding_1', 'embedding_2')\n# UMAP_df$type <- rep(\"UMAP\", nrow(data_pca))\n# \n# ## Phate\n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# \n# tree_phate_fit <- phate(data_pca, 5)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding) \n# PHATE_df <- PHATE_df %>% select(\"PHATE1\", \"PHATE2\")\n# names(PHATE_df) <- c('embedding_1', 'embedding_2')\n# PHATE_df$type <- rep(\"PHATE\", nrow(data_pca))\n# \n# ## TriMAP\n# \n# source(\"functions_tri_hex_TriMAP_without_pc.R\", local = TRUE)\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# tem_dir <- tempdir()\n# \n# Fit_TriMAP_data(data_pca, tem_dir)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# names(df_TriMAP) <- c('embedding_1', 'embedding_2')\n# df_TriMAP$type <- rep(\"TriMAP\", nrow(data_pca))\n# \n#     \n# ## PaCMAP\n# source(\"functions_tri_hex_PaCMAP_without_pc.R\", local = TRUE)\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# \n# tem_dir <- tempdir()\n# \n# Fit_PacMAP_data(data_pca, tem_dir)\n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# names(df_PaCMAP) <- c('embedding_1', 'embedding_2')\n# df_PaCMAP$type <- rep(\"PaCMAP\", nrow(data_pca))\n# \n# expample_1 <- bind_rows(tSNE_df, UMAP_df, PHATE_df, df_TriMAP, df_PaCMAP)\n# expample_1\n# write_csv(expample_1, \"Example_1.csv\")\n\nexample_1 <- read_csv(\"Example_1.csv\")\n\nexpample_1_with_label <- example_1 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = rep(df_2$class, 5))) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~type, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\")\n\nexpample_1_with_label\n\n\n\n\n\nFigure¬†2: 2D embedding by t-SNE, UMAP, PHATE, PaCMAP, and TriMAP\n\n\n\n\nAccording to Figure¬†2, the 2D embedding plots show the two clusters, although with very different shapes. With PHATE, TriMAP, and UMAP, the local structure dominates. Also, clumps (random noise) exist within each cluster in t-SNE and PaCMAP.\nLet‚Äôs consider more complex examples. First, I‚Äôll evaluate five Gaussian clusters in 4D, as shown in Figure¬†3. Distances between clusters and cluster size are the same in this example.\n\n\nCode\nset.seed(202303002)\ndf1 <- tibble::tibble(x=rnorm(100, mean = 0, sd = 0.05), y=rnorm(100, mean = 0, sd = 0.05), z=rnorm(100, mean = 0, sd = 0.05), w=rnorm(100, mean = 0, sd = 0.05))\ndf1$class <- rep(\"Class 1\",100)\n\ndf2 <- tibble::tibble(x=rnorm(100, mean = 1, sd = 0.05), y=rnorm(100, mean = 0, sd = 0.05), z=rnorm(100, mean = 0, sd = 0.05), w=rnorm(100, mean = 0, sd = 0.05))\ndf2$class <- rep(\"Class 2\",100)\n\ndf3 <- tibble::tibble(x=rnorm(100, mean = 0, sd = 0.05), y=rnorm(100, mean = 1, sd = 0.05), z=rnorm(100, mean = 0, sd = 0.05), w=rnorm(100, mean = 0, sd = 0.05))\ndf3$class <- rep(\"Class 3\",100)\n\ndf4 <- tibble::tibble(x=rnorm(100, mean = 0, sd = 0.05), y=rnorm(100, mean = 0, sd = 0.05), z=rnorm(100, mean = 1, sd = 0.05), w=rnorm(100, mean = 0, sd = 0.05))\ndf4$class <- rep(\"Class 4\",100)\n\ndf5 <- tibble::tibble(x=rnorm(100, mean = 0, sd = 0.05), y=rnorm(100, mean = 0, sd = 0.05), z=rnorm(100, mean = 0, sd = 0.05), w=rnorm(100, mean = 1, sd = 0.05))\ndf5$class <- rep(\"Class 5\",100)\n\ndf_2_n <- rbind(df1, df2, df3, df4, df5)\ndf_2_n <- df_2_n %>% rename(x1 = x, x2 = y, x3 = z, x4 = w)\n\ndata_pca_n <- df_2_n %>%\n  select(where(is.numeric))\n\nlangevitour(data_pca_n, group = df_2_n$class, levelColors = c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\"))\n\n\n\n\n\nFigure¬†3: Original data of five Gaussian clusters in 4D.\n\n\n\nLet‚Äôs visualize 2D embedding with default neighborhood parameter value.\n\n\nCode\n# ## tSNE\n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# tSNE_fit <- data_pca_n %>%\n#   Rtsne(perplexity = 30)\n# \n# tSNE_df <- tSNE_fit$Y %>%\n#   as.data.frame()\n# names(tSNE_df) <- c('embedding_1', 'embedding_2')\n# tSNE_df$type <- rep(\"tSNE\", nrow(data_pca_n))\n# \n# ## UMAP\n# \n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# UMAP_fit <- data_pca_n %>%\n#   umap(n_neighbors = 15)\n# \n# UMAP_df <- UMAP_fit$layout %>%\n#   as.data.frame()\n# names(UMAP_df) <- c('embedding_1', 'embedding_2')\n# UMAP_df$type <- rep(\"UMAP\", nrow(data_pca_n))\n# \n# ## Phate\n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# \n# tree_phate_fit <- phate(data_pca_n, 5)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding)\n# PHATE_df <- PHATE_df %>% select(\"PHATE1\", \"PHATE2\")\n# names(PHATE_df) <- c('embedding_1', 'embedding_2')\n# PHATE_df$type <- rep(\"PHATE\", nrow(data_pca_n))\n# \n# ## TriMAP\n# \n# source(\"functions_tri_hex_TriMAP_without_pc.R\", local = TRUE)\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# tem_dir <- tempdir()\n# \n# Fit_TriMAP_data(data_pca_n, tem_dir)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# names(df_TriMAP) <- c('embedding_1', 'embedding_2')\n# df_TriMAP$type <- rep(\"TriMAP\", nrow(data_pca_n))\n# \n# \n# ## PaCMAP\n# source(\"functions_tri_hex_PaCMAP_without_pc.R\", local = TRUE)\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# \n# tem_dir <- tempdir()\n# \n# Fit_PacMAP_data(data_pca_n, tem_dir)\n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# names(df_PaCMAP) <- c('embedding_1', 'embedding_2')\n# df_PaCMAP$type <- rep(\"PaCMAP\", nrow(data_pca_n))\n# \n# expample_2 <- bind_rows(tSNE_df, UMAP_df, PHATE_df, df_TriMAP, df_PaCMAP)\n# expample_2\n# write_csv(expample_2, \"Example_2.csv\")\n\nexample_2 <- read_csv(\"Example_2.csv\")\n\nexample_2_with_label <- example_2 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = rep(df_2_n$class, 5))) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~type, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\")\n\nexample_2_with_label\n\n\n\n\n\nFigure¬†4: 2D embedding by t-SNE, UMAP, PHATE, PaCMAP, and TriMAP\n\n\n\n\nFive well-separated clusters are in low-dimensional space, with different shapes in each NLDR technique (see Figure¬†4). The next question that comes to your mind is whether the distance in high-dimensional space preserves in low-dimension (see the blog).\nUntil now, we considered examples only with clusters. Let‚Äôs see what happens to a 2D curvilinear in 4D space.\n\n\nCode\nset.seed(202303003)\nx <- runif(500, 0, 2)\ny <- -(x^3 + runif(500, 0, 3)) + runif(500, 0, 0.5)\nz <- rep(0, 500) + runif(500, -1, 1)\nw <- rep(0, 500) - runif(500, -1, 1)\n\ndf_2_n1 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w)\n\n\nlangevitour(df_2_n1)\n\n\n\n\n\nFigure¬†5: Original data of 2D curvilinear in 4D.\n\n\n\nAs shown in Figure¬†6, 2D embedding in PHATE preserves the local structure, while others have misleading results. For example, in TriMAP, 2D curvilinear is not preserved with the default neighborhood value. On the other hand, PaCMAP, UMAP, and t-SNE have the geometry of a 2D curvilinear, although clumpy points mislead the result.\n\n\nCode\n# ## tSNE\n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# tSNE_fit <- df_2_n1 %>%\n#   Rtsne(perplexity = 30)\n# \n# tSNE_df <- tSNE_fit$Y %>%\n#   as.data.frame()\n# names(tSNE_df) <- c('embedding_1', 'embedding_2')\n# tSNE_df$type <- rep(\"tSNE\", nrow(df_2_n1))\n# \n# ## UMAP\n# \n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# UMAP_fit <- df_2_n1 %>%\n#   umap(n_neighbors = 15)\n# \n# UMAP_df <- UMAP_fit$layout %>%\n#   as.data.frame()\n# names(UMAP_df) <- c('embedding_1', 'embedding_2')\n# UMAP_df$type <- rep(\"UMAP\", nrow(df_2_n1))\n# \n# ## Phate\n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# \n# tree_phate_fit <- phate(df_2_n1, 5)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding)\n# PHATE_df <- PHATE_df %>% select(\"PHATE1\", \"PHATE2\")\n# names(PHATE_df) <- c('embedding_1', 'embedding_2')\n# PHATE_df$type <- rep(\"PHATE\", nrow(df_2_n1))\n# \n# ## TriMAP\n# \n# source(\"functions_tri_hex_TriMAP_without_pc.R\", local = TRUE)\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# tem_dir <- tempdir()\n# \n# Fit_TriMAP_data(df_2_n1, tem_dir)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# names(df_TriMAP) <- c('embedding_1', 'embedding_2')\n# df_TriMAP$type <- rep(\"TriMAP\", nrow(df_2_n1))\n# \n# \n# ## PaCMAP\n# source(\"functions_tri_hex_PaCMAP_without_pc.R\", local = TRUE)\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# \n# tem_dir <- tempdir()\n# \n# Fit_PacMAP_data(df_2_n1, tem_dir)\n# random_num2 <- runif(1, min = 1, max = 10000000)\n# set.seed(random_num2)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# names(df_PaCMAP) <- c('embedding_1', 'embedding_2')\n# df_PaCMAP$type <- rep(\"PaCMAP\", nrow(df_2_n1))\n# \n# expample_3 <- bind_rows(tSNE_df, UMAP_df, PHATE_df, df_TriMAP, df_PaCMAP)\n# expample_3\n# write_csv(expample_3, \"Example_3.csv\")\n\nexample_3 <- read_csv(\"Example_3.csv\")\n\nexample_3_with_label <- example_3 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2)) +\n    geom_point() +\n  theme(aspect.ratio = 1) + facet_wrap(~type, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\")\n\nexample_3_with_label\n\n\n\n\n\nFigure¬†6: 2D embedding by t-SNE, UMAP, PHATE, PaCMAP, and TriMAP\n\n\n\n\nSo far, you have an idea that the default value of the neighborhood parameter is only sometimes produced better visualization."
  },
  {
    "objectID": "posts/blog_6/index.html#is-it-possible-to-find-an-effective-value-for-the-neighborhood-parameter",
    "href": "posts/blog_6/index.html#is-it-possible-to-find-an-effective-value-for-the-neighborhood-parameter",
    "title": "Is neighborhood parameter really matter in non-linear dimensionality reduction (NLDR) techniques?",
    "section": "Is it possible to find an effective value for the neighborhood parameter?",
    "text": "Is it possible to find an effective value for the neighborhood parameter?\nLaurens van der Maaten suggested that perplexity values in the range (5 - 50) are appropriate for larger / denser data sets. Let‚Äôs see the outputs for the simple and complex example defined earlier with different perplexity values.\n\n\nCode\ntSNE_by_perplexity <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(tSNE_by_perplexity) <- c(\"embedding_1\", \"embedding_2\", \"perplexity\")\ntSNE_by_perplexity$perplexity <- as.character(tSNE_by_perplexity$perplexity)\n                                \n# Create the loop.vector \nloop.vector2 <- c(2, 5, 30, 50, 65)\n                                \nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n\n  perplexity <- loop.vector2[i]\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  tSNE_fit <- data_pca %>%\n    Rtsne(perplexity = perplexity)\n\n  tSNE_df <- tSNE_fit$Y %>%\n    as.data.frame()\n  names(tSNE_df) <- c('embedding_1', 'embedding_2')\n  tSNE_df$perplexity <- rep(paste0(\"Perplexity: \",perplexity),nrow(data_pca))\n  \n  tSNE_by_perplexity <- bind_rows(tSNE_by_perplexity,tSNE_df)\n}\n\n\nplot_tSNE_by_perplexity <- tSNE_by_perplexity %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = rep(df_2$class, 5))) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~perplexity, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity\n\n\n\n\n\nFigure¬†7: 2D embedding of t-SNE by perplexity.\n\n\n\n\nThe two clusters are shown within the range of (5-50) but with different shapes (see Figure¬†7). Outside that range, things get weird when the perplexity value is less than 5. With perplexity 2, local variations dominate.\n\n\nCode\ntSNE_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(tSNE_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"perplexity\")\ntSNE_by_perplexity1$perplexity <- as.character(tSNE_by_perplexity1$perplexity)\n                                \n# Create the loop.vector \nloop.vector2 <- c(2, 5, 30, 50, 100, 150)\n                                \nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n\n  perplexity <- loop.vector2[i]\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  tSNE_fit <- data_pca_n %>%\n    Rtsne(perplexity = perplexity)\n\n  tSNE_df <- tSNE_fit$Y %>%\n    as.data.frame()\n  names(tSNE_df) <- c('embedding_1', 'embedding_2')\n  tSNE_df$perplexity <- rep(paste0(\"Perplexity: \",perplexity),nrow(data_pca_n))\n  \n  tSNE_by_perplexity1 <- bind_rows(tSNE_by_perplexity1,tSNE_df)\n}\n\ntSNE_by_perplexity1$perplexity_f = factor(tSNE_by_perplexity1$perplexity, levels=c('Perplexity: 2', 'Perplexity: 5', 'Perplexity: 30', 'Perplexity: 50', 'Perplexity: 100', 'Perplexity: 150'))\n\nplot_tSNE_by_perplexity1 <- tSNE_by_perplexity1 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = rep(df_2_n$class, 6))) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~perplexity_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†8: 2D embedding of t-SNE by perplexity.\n\n\n\n\nThe local structure dominates when the perplexity is 2 (see Figure¬†8). On the other hand, even though the perplexity is greater than 100, the five clusters are well-separated. Therefore, the range of perplexity defined by the author can be useful for getting an initial value; an effective value for the perplexity can vary according to the data set.\n\n\nCode\ntSNE_by_perplexity2 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(tSNE_by_perplexity2) <- c(\"embedding_1\", \"embedding_2\", \"perplexity\")\ntSNE_by_perplexity2$perplexity <- as.character(tSNE_by_perplexity2$perplexity)\n                                \n# Create the loop.vector \nloop.vector2 <- c(2, 5, 30, 50, 100, 150)\n                                \nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n\n  perplexity <- loop.vector2[i]\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  tSNE_fit <- df_2_n1 %>%\n    Rtsne(perplexity = perplexity)\n\n  tSNE_df <- tSNE_fit$Y %>%\n    as.data.frame()\n  names(tSNE_df) <- c('embedding_1', 'embedding_2')\n  tSNE_df$perplexity <- rep(paste0(\"Perplexity: \",perplexity),nrow(df_2_n1))\n  \n  tSNE_by_perplexity2 <- bind_rows(tSNE_by_perplexity2,tSNE_df)\n}\n\ntSNE_by_perplexity2$perplexity_f = factor(tSNE_by_perplexity2$perplexity, levels=c('Perplexity: 2', 'Perplexity: 5', 'Perplexity: 30', 'Perplexity: 50', 'Perplexity: 100', 'Perplexity: 150'))\n\nplot_tSNE_by_perplexity2 <- tSNE_by_perplexity2 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2)) +\n    geom_point() +\n  theme(aspect.ratio = 1) + facet_wrap(~perplexity_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity2\n\n\n\n\n\nFigure¬†9: 2D embedding of t-SNE by perplexity.\n\n\n\n\nYou can see in Figure¬†9, except when the perplexity is 30, the shape is not preserved in low-dimensional space.\nNote: For the t-SNE algorithm to operate properly, the perplexity really should be smaller than the number of points.\n\nEffective value for perplexity (neighborhood parameter of t-SNE)\nAccording to Nikolay Oskolkov, the \\(\\text{Perplexity} ‚àº N^{(1/2)}\\) where \\(N\\) is the number of observations.\n\\[\n\\begin{equation}\n    \\text{Perplexity} = k * N^{(1/2)}.\n   \\label{eq:eq3}\n  \\end{equation}\n\\]\nIn the simplest scenario, we take \\(k = 1\\). The effective values for the example data sets is shown below.\n\nTable 2: Effective perplexity value for example data sets\n\n\n\n\n\n\nData set\nEffective perplexity value\n\n\n\n\nTwo clusters in a 2D plane\n14.14214\n\n\nFive clusters in 4D\n22.36068\n\n\n2D curvillinear in 4D\n22.36068\n\n\n\n\n\nCode\nN1 <- dim(data_pca)[1]\nopt_perplexity1 <- sqrt(N1)\n\n\nN2 <- dim(data_pca_n)[1]\nopt_perplexity2 <- sqrt(N2)\n\n\nN3 <- dim(df_2_n1)[1]\nopt_perplexity3 <- sqrt(N3)\n\n\nLet‚Äôs visualize and see whether these values are actually effective.\n\n\nCode\nrandom_num2 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num2)\ntSNE_fit <- data_pca %>%\n  Rtsne(perplexity = opt_perplexity1)\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()\nnames(tSNE_df) <- c('embedding_1', 'embedding_2')\n\nplot_tSNE_by_effect_perplexity <- tSNE_df %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = df_2$class)) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_effect_perplexity\n\n\n\n\n\nFigure¬†10: 2D embedding of t-SNE for the effetive perplexity value.\n\n\n\n\n\n\nCode\nrandom_num2 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num2)\ntSNE_fit <- data_pca_n %>%\n  Rtsne(perplexity = opt_perplexity2)\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()\nnames(tSNE_df) <- c('embedding_1', 'embedding_2')\n\nplot_tSNE_by_effect_perplexity1 <- tSNE_df %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = df_2_n$class)) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_effect_perplexity1\n\n\n\n\n\nFigure¬†11: 2D embedding of t-SNE for the effetive perplexity value.\n\n\n\n\n\n\nCode\nrandom_num2 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num2)\ntSNE_fit <- df_2_n1 %>%\n  Rtsne(perplexity = opt_perplexity3)\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()\nnames(tSNE_df) <- c('embedding_1', 'embedding_2')\n\nplot_tSNE_by_effect_perplexity2 <- tSNE_df %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2)) +\n    geom_point() +\n  theme(aspect.ratio = 1) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_effect_perplexity2\n\n\n\n\n\nFigure¬†12: 2D embedding of t-SNE for the effetive perplexity value.\n\n\n\n\nRegarding Figure¬†10, Figure¬†11, and Figure¬†12, you can see that these values are not the best, but to begin the analysis, we can use these as initial values.\nThe authors do not officially define a method of finding an effective value for the neighborhood parameter in other NLDR techniques. Therefore, we have to visualize and see the best value to consider.\nLet‚Äôs compare the outputs of other NLDR techniques.\n\n\nCode\nUMAP_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(UMAP_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nUMAP_by_perplexity1$n_neighbors <- as.character(UMAP_by_perplexity1$n_neighbors)\n                                \n# Create the loop.vector \nloop.vector2 <- c(2, 5, 15, 30, 50, 100)\n                                \nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n\n  n_neighbors <- loop.vector2[i]\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  UMAP_fit <- data_pca_n %>%\n    umap(n_neighbors = n_neighbors)\n\n  UMAP_df <- UMAP_fit$layout %>%\n    as.data.frame()\n  names(UMAP_df) <- c('embedding_1', 'embedding_2')\n  UMAP_df$n_neighbors <- rep(paste0(\"n_neighbors: \", n_neighbors),nrow(data_pca_n))\n  \n  UMAP_by_perplexity1 <- bind_rows(UMAP_by_perplexity1,UMAP_df)\n}\n\nUMAP_by_perplexity1$n_neighbors_f = factor(UMAP_by_perplexity1$n_neighbors, levels=c('n_neighbors: 2', 'n_neighbors: 5', 'n_neighbors: 15', 'n_neighbors: 30', 'n_neighbors: 50', 'n_neighbors: 100'))\n\nplot_tSNE_by_perplexity1 <- UMAP_by_perplexity1 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2, color = rep(df_2_n$class, 6))) +\n    geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~n_neighbors_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†13: 2D embedding of UMAP by n_neighbors.\n\n\n\n\nThe five clusters are nicely separated except when the number of neighbors is 2 in UMAP (see Figure¬†13). In n-neighbors: 2, the original clusters are divided into small clusters, representing a misleading result. Also, the distance between clusters (change global structure) and the shapes of the clusters vary as the value of n_neighbors increases.\n\n\nCode\nUMAP_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(UMAP_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nUMAP_by_perplexity1$n_neighbors <- as.character(UMAP_by_perplexity1$n_neighbors)\n                                \n# Create the loop.vector \nloop.vector2 <- c(2, 5, 15, 30, 50, 100)\n                                \nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n\n  n_neighbors <- loop.vector2[i]\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  UMAP_fit <- df_2_n1 %>%\n    umap(n_neighbors = n_neighbors)\n\n  UMAP_df <- UMAP_fit$layout %>%\n    as.data.frame()\n  names(UMAP_df) <- c('embedding_1', 'embedding_2')\n  UMAP_df$n_neighbors <- rep(paste0(\"n_neighbors: \", n_neighbors),nrow(df_2_n1))\n  \n  UMAP_by_perplexity1 <- bind_rows(UMAP_by_perplexity1,UMAP_df)\n}\n\nUMAP_by_perplexity1$n_neighbors_f = factor(UMAP_by_perplexity1$n_neighbors, levels=c('n_neighbors: 2', 'n_neighbors: 5', 'n_neighbors: 15', 'n_neighbors: 30', 'n_neighbors: 50', 'n_neighbors: 100'))\n\nplot_tSNE_by_perplexity1 <- UMAP_by_perplexity1 %>%\n    ggplot(aes(x = embedding_1,\n               y = embedding_2)) +\n    geom_point() +\n  theme(aspect.ratio = 1) + facet_wrap(~n_neighbors_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†14: 2D embedding of UMAP by n_neighbors.\n\n\n\n\nAs shown in Figure¬†15, in TriMAP, the local structure dominates when the number of neighbors is 2 and 100. There are some sub-clusters of main clusters.\n\n\nCode\nTriMAP_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(TriMAP_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nTriMAP_by_perplexity1$n_neighbors <- as.character(TriMAP_by_perplexity1$n_neighbors)\n\n# Create the loop.vector \nloop.vector2 <- c(2, 5, 12, 30, 50, 100)\n\nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n  \n  n_inliers_n <- as.integer(loop.vector2[i])\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  source(\"functions_tri_hex_TriMAP_without_pc.R\", local = TRUE)\n  source_python(\"Fit_TriMAP_code.py\")\n  \n  #n_inliers_n <- as.integer(12)\n  n_outliers_n <- as.integer(4)\n  n_random_n <- as.integer(3)\n  \n  tem_dir <- tempdir()\n  \n  Fit_TriMAP_data(data_pca_n, tem_dir)\n  \n  path <- file.path(tem_dir, \"df_2_without_class.csv\")\n  path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n  \n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  \n  Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n  \n  TriMAP_df <- read.csv(path2)\n  names(TriMAP_df) <- c('embedding_1', 'embedding_2')\n  TriMAP_df$n_inliers <- rep(paste0(\"n_inliers: \", n_inliers_n),nrow(data_pca_n))\n  \n  TriMAP_by_perplexity1 <- bind_rows(TriMAP_by_perplexity1,TriMAP_df)\n}\n\nTriMAP_by_perplexity1$n_inliers_f = factor(TriMAP_by_perplexity1$n_inliers, levels=c('n_inliers: 2', 'n_inliers: 5', 'n_inliers: 12', 'n_inliers: 30', 'n_inliers: 50', 'n_inliers: 100'))\n\nplot_tSNE_by_perplexity1 <- TriMAP_by_perplexity1 %>%\n  ggplot(aes(x = embedding_1,\n             y = embedding_2, color = rep(df_2_n$class, 6))) +\n  geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~n_inliers_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†15: 2D embedding of TriMAP by knn.\n\n\n\n\nIn TriMAP, 2D curvilinear changes from a cheese slice to an umbrella (see Figure¬†16). The local structure is not preserved as expected.\n\n\nCode\nTriMAP_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(TriMAP_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nTriMAP_by_perplexity1$n_neighbors <- as.character(TriMAP_by_perplexity1$n_neighbors)\n\n# Create the loop.vector \nloop.vector2 <- c(2, 5, 12, 30, 50, 100)\n\nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n  \n  n_inliers_n <- as.integer(loop.vector2[i])\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  source(\"functions_tri_hex_TriMAP_without_pc.R\", local = TRUE)\n  source_python(\"Fit_TriMAP_code.py\")\n\n  #n_inliers_n <- as.integer(12)\n  n_outliers_n <- as.integer(4)\n  n_random_n <- as.integer(3)\n\n  tem_dir <- tempdir()\n\n  Fit_TriMAP_data(df_2_n1, tem_dir)\n\n  path <- file.path(tem_dir, \"df_2_without_class.csv\")\n  path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n\n  Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n\n  TriMAP_df <- read.csv(path2)\n  names(TriMAP_df) <- c('embedding_1', 'embedding_2')\n  TriMAP_df$n_inliers <- rep(paste0(\"n_inliers: \", n_inliers_n),nrow(df_2_n1))\n  \n  TriMAP_by_perplexity1 <- bind_rows(TriMAP_by_perplexity1,TriMAP_df)\n}\n\nTriMAP_by_perplexity1$n_inliers_f = factor(TriMAP_by_perplexity1$n_inliers, levels=c('n_inliers: 2', 'n_inliers: 5', 'n_inliers: 12', 'n_inliers: 30', 'n_inliers: 50', 'n_inliers: 100'))\n\nplot_tSNE_by_perplexity1 <- TriMAP_by_perplexity1 %>%\n  ggplot(aes(x = embedding_1,\n             y = embedding_2)) +\n  geom_point() +\n  theme(aspect.ratio = 1) + facet_wrap(~n_inliers_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†16: 2D embedding of TriMAP by knn.\n\n\n\n\nIn PaCMAP, with knn: 2, local variations dominate, dividing each cluster into small clusters (see Figure¬†17). However, when the knn value is greater than 5, the five clusters preserve the global structure, while the shape of the clusters changes a bit.\n\n\nCode\nPaCMAP_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(PaCMAP_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nPaCMAP_by_perplexity1$knn <- as.character(PaCMAP_by_perplexity1$knn)\n\n# Create the loop.vector \nloop.vector2 <- c(2, 5, 10, 30, 50, 100)\n\nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n  \n  knn_n <- as.integer(loop.vector2[i])\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  source(\"functions_tri_hex_PaCMAP_without_pc.R\", local = TRUE)\n  source_python(\"Fit_PacMAP_code.py\")\n  \n  init_n <- \"random\"\n  MN_ratio_n <- 0.5\n  FP_ratio_n <- 2.0\n\n  tem_dir <- tempdir()\n\n  Fit_PacMAP_data(data_pca_n, tem_dir)\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  path <- file.path(tem_dir, \"df_2_without_class.csv\")\n  path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n  Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n  \n  PaCMAP_df <- read.csv(path2)\n  \n  names(PaCMAP_df) <- c('embedding_1', 'embedding_2')\n  PaCMAP_df$knn <- rep(paste0(\"knn: \", knn_n),nrow(data_pca_n))\n  \n  PaCMAP_by_perplexity1 <- bind_rows(PaCMAP_by_perplexity1,PaCMAP_df)\n}\n\nPaCMAP_by_perplexity1$knn_f = factor(PaCMAP_by_perplexity1$knn, levels=c('knn: 2', 'knn: 5', 'knn: 10', 'knn: 30', 'knn: 50', 'knn: 100'))\n\nplot_tSNE_by_perplexity1 <- PaCMAP_by_perplexity1 %>%\n  ggplot(aes(x = embedding_1,\n             y = embedding_2, color = rep(df_2_n$class, 6))) +\n  geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~knn_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†17: 2D embedding of PaCMAP by n_neighbors.\n\n\n\n\nAnother surprise‚Ä¶when knn is 2, the 2D curvilinear looks like a tree branch (see Figure¬†18). Also, as the knn value increases, the 2D curvilinear becomes more clumpy, misleading the structure.\n\n\nCode\nPaCMAP_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(PaCMAP_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nPaCMAP_by_perplexity1$knn <- as.character(PaCMAP_by_perplexity1$knn)\n\n# Create the loop.vector \nloop.vector2 <- c(2, 5, 10, 30, 50, 100)\n\nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n  \n  knn_n <- as.integer(loop.vector2[i])\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  source(\"functions_tri_hex_PaCMAP_without_pc.R\", local = TRUE)\n  source_python(\"Fit_PacMAP_code.py\")\n  \n  init_n <- \"random\"\n  MN_ratio_n <- 0.5\n  FP_ratio_n <- 2.0\n  \n  tem_dir <- tempdir()\n  \n  Fit_PacMAP_data(df_2_n1, tem_dir)\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  path <- file.path(tem_dir, \"df_2_without_class.csv\")\n  path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n  Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n  \n  PaCMAP_df <- read.csv(path2)\n  \n  names(PaCMAP_df) <- c('embedding_1', 'embedding_2')\n  PaCMAP_df$knn <- rep(paste0(\"knn: \", knn_n),nrow(df_2_n1))\n  \n  PaCMAP_by_perplexity1 <- bind_rows(PaCMAP_by_perplexity1,PaCMAP_df)\n}\n\nPaCMAP_by_perplexity1$knn_f = factor(PaCMAP_by_perplexity1$knn, levels=c('knn: 2', 'knn: 5', 'knn: 10', 'knn: 30', 'knn: 50', 'knn: 100'))\n\nplot_tSNE_by_perplexity1 <- PaCMAP_by_perplexity1 %>%\n  ggplot(aes(x = embedding_1,\n             y = embedding_2)) +\n  geom_point() +\n  theme(aspect.ratio = 1) + facet_wrap(~knn_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†18: 2D embedding of PaCMAP by n_neighbors.\n\n\n\n\nAs seen in Figure¬†19, and Figure¬†20, we have a surprising result. After the default neighborhood value (5) of PHATE, the visualizations are the same as the default neighborhood value.\n\n\nCode\nPHATE_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(PHATE_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nPHATE_by_perplexity1$knn <- as.character(PHATE_by_perplexity1$knn)\n\n# Create the loop.vector \nloop.vector2 <- c(2, 5, 10, 30, 50, 100)\n\nfor (i in 1:length(loop.vector2)) { # Loop over loop.vector\n  \n  knn_n <- as.integer(loop.vector2[i])\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  \n  tree_phate_fit <- phate(data_pca_n, knn_n)\n  \n  PHATE_df <- as.data.frame(tree_phate_fit$embedding)\n  PHATE_df <- PHATE_df %>% select(\"PHATE1\", \"PHATE2\")\n  names(PHATE_df) <- c('embedding_1', 'embedding_2')\n  #PHATE_df$type <- rep(\"PHATE\", nrow(df_2_n1))\n\n  names(PHATE_df) <- c('embedding_1', 'embedding_2')\n  PHATE_df$knn <- rep(paste0(\"knn: \", knn_n),nrow(data_pca_n))\n  \n  PHATE_by_perplexity1 <- bind_rows(PHATE_by_perplexity1,PHATE_df)\n}\n\nPHATE_by_perplexity1$knn_f = factor(PHATE_by_perplexity1$knn, levels=c('knn: 2', 'knn: 5', 'knn: 10', 'knn: 30', 'knn: 50', 'knn: 100'))\n\nplot_tSNE_by_perplexity1 <- PHATE_by_perplexity1 %>%\n  ggplot(aes(x = embedding_1,\n             y = embedding_2, color = rep(df_2_n$class, 6))) +\n  geom_point() +\n  scale_color_manual(values=c(\"#1f78b4\", \"#e31a1c\", \"#33a02c\", \"#ff7f00\", \"#6a3d9a\")) +\n  guides(color = guide_legend(title = \"Class\")) +\n  theme(aspect.ratio = 1) + facet_wrap(~knn_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†19: 2D embedding of PHATE by knn.\n\n\n\n\n\n\nCode\nPHATE_by_perplexity1 <- data.frame(matrix(ncol = 3, nrow = 0))\nnames(PHATE_by_perplexity1) <- c(\"embedding_1\", \"embedding_2\", \"n_neighbors\")\nPHATE_by_perplexity1$knn <- as.character(PHATE_by_perplexity1$knn)\n\n# Create the loop.vector \nloop.vector2 <- c(2, 5, 10, 30, 50, 100)\n\nfor (i in 1:length(loop.vector2)) {  # Loop over loop.vector\n  \n  knn_n <- as.integer(loop.vector2[i])\n  random_num2 <- runif(1, min = 1, max = 10000000)\n  set.seed(random_num2)\n  \n  tree_phate_fit <- phate(df_2_n1, knn_n)\n  \n  PHATE_df <- as.data.frame(tree_phate_fit$embedding)\n  PHATE_df <- PHATE_df %>% select(\"PHATE1\", \"PHATE2\")\n  names(PHATE_df) <- c('embedding_1', 'embedding_2')\n  #PHATE_df$type <- rep(\"PHATE\", nrow(df_2_n1))\n\n  names(PHATE_df) <- c('embedding_1', 'embedding_2')\n  PHATE_df$knn <- rep(paste0(\"knn: \", knn_n),nrow(df_2_n1))\n  \n  PHATE_by_perplexity1 <- bind_rows(PHATE_by_perplexity1,PHATE_df)\n\n}\n\nPHATE_by_perplexity1$knn_f = factor(PHATE_by_perplexity1$knn, levels=c('knn: 2', 'knn: 5', 'knn: 10', 'knn: 30', 'knn: 50', 'knn: 100'))\n\nplot_tSNE_by_perplexity1 <- PHATE_by_perplexity1 %>%\n  ggplot(aes(x = embedding_1,\n             y = embedding_2)) +\n  geom_point() +\n  theme(aspect.ratio = 1) + facet_wrap(~knn_f, scales = \"free\", ncol = 3) + xlab(\"embedding 1\") + ylab(\"embedding 2\") \n\nplot_tSNE_by_perplexity1\n\n\n\n\n\nFigure¬†20: 2D embedding of PHATE by knn."
  },
  {
    "objectID": "posts/blog_6/index.html#conclusion",
    "href": "posts/blog_6/index.html#conclusion",
    "title": "Is neighborhood parameter really matter in non-linear dimensionality reduction (NLDR) techniques?",
    "section": "Conclusion",
    "text": "Conclusion\nThe effective value of the neighborhood parameter depends on the data set. Therefore, we must choose the most effective one by evaluating the outputs for different neighborhood parameter values."
  }
]