[
  {
    "objectID": "posts/blog_1/index.html",
    "href": "posts/blog_1/index.html",
    "title": "Can the tSNE results be reproducible?",
    "section": "",
    "text": "If you are an R user struggling to access HARPS GTO sample dataset and have the question that Can‚Äôt we reproduce the t-distributed stochastic neighbor embedding (tSNE) results that already exist? In their mind, this blog is for you. Today, I will introduce my workflow of accessing processed data from original data, downloading the processed data, and data processing to initialize cluster labels and show my surprising findings on reproducing tSNE results.\nThis example is relevant to the paper, ‚ÄúDissecting stellar chemical abundance space with t-SNE‚Äù (Anders et al.¬†2018, Astronomy & Astrophysics 619, A125).\nFor simplicity, I added all the codes separately and linked them through the process.\nPlease note that you will need to \\(\\color{green}{\\text{install python library}}\\) called open_data, which you can find in open_data.py as the initial step."
  },
  {
    "objectID": "posts/blog_1/index.html#data-processing-steps-already-followed-by-open_data-library",
    "href": "posts/blog_1/index.html#data-processing-steps-already-followed-by-open_data-library",
    "title": "Can the tSNE results be reproducible?",
    "section": "Data processing steps already followed by open_data library",
    "text": "Data processing steps already followed by open_data library\nThe original data can be found in the GitHub repository of the authors as DelgadoMena2017.fits, which includes some existing cluster labels and abundance determination for \\(\\color{blue}{\\text{Mg, Al, Si, Ca, TiI, Fe, Cu, Zn, Sr, Y, ZrII, Ce, and Ba}}\\). This file contains details regarding \\(\\color{red}{\\text{1059}}\\) stars.\nThen, how do we get only \\(\\color{red}{\\text{530}}\\) stars? ü§î\nLet‚Äôs investigate‚Ä¶\nAccording to the authors, the sample needed to be analyzed in a more restricted temperature range to obtain reliable tSNE abundance maps. The reason is that specific abundance trends dominate underlying temperature trends. Therefore,\n\nchoose an effective temperature range of \\(\\color{brown}{5300 \\text{ K} < T_{eff} < 6000 \\text{ K}}\\).\n\nOnly \\(\\color{green}{539}\\) stars were satisfied in this step. Next,\n\nexclude stars with \\(\\color{brown}{\\text{log } g_{HIP}} < 3\\) which remove \\(\\color{green}{\\text{one}}\\) star.\n\nThen,\n\nselect successful abundance determination for \\(\\color{brown}{\\text{Mg, Al, Si, Ca, TiI, Fe, Cu, Zn, Sr, Y, ZrII, Ce, and Ba}}\\) which use as input for tSNE.\n\nOnly \\(\\color{green}{533}\\) stars have remained because others contain missing values. Furthermore,\n\nto compensate for the fact that tSNE does not take into account individual (heteroscedastic) uncertainties in the data, the authors followed the approach of Hogg et al.¬†(2016) and rescaled each abundance by the median uncertainty in that element, assuming an abundance uncertainty floor of 0.03 dex. Additionally,\nremove stars that did not converge by using the age determination code, StarHose code.\n\nThe \\(\\color{green}{3}\\) stars are discarded.\nThere are only \\(\\color{green}{530}\\) stars in our final sample. üëè\nNote: You do not need to consider the above steps because the open_data package has already done it for us."
  },
  {
    "objectID": "posts/blog_1/index.html#our-workflow",
    "href": "posts/blog_1/index.html#our-workflow",
    "title": "Can the tSNE results be reproducible?",
    "section": "Our workflow",
    "text": "Our workflow\nLet‚Äôs begin‚Ä¶ ü§ì\n\nTo download the processed data as a CSV file and save it locally, you need to run download_original.py,\nRun this code to process data further as indicated in the original paper to compute missing variables, new variables, cluster labels and produce plots of authors t-SNE results,\nRun this code to conduct our own t-SNE results.\n\nHere, I used the \\(\\color{red}{\\text{Perplexity parameter as 40}}\\) authors already mentioned that in the paper.\nHere is the output..ü§ó"
  },
  {
    "objectID": "posts/blog_1/index.html#acknowledge",
    "href": "posts/blog_1/index.html#acknowledge",
    "title": "Can the tSNE results be reproducible?",
    "section": "Acknowledge",
    "text": "Acknowledge\nI am grateful for Prof.¬†Dianne Cook who suggested me as an exercise for me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Team",
    "section": "",
    "text": "I am a first year PhD student in Department of Econometrics and Business Statistics at the Monash University, Australia. This website contains the findings of my PhD research project.\nMy supervisors are:\n\nProfessor Dianne Cook\nDr.¬†Michael Lydeamore\nDr.¬†Paul Harrison\nDr.¬†Thiyanga S. Talagala"
  },
  {
    "objectID": "Blogs.html",
    "href": "Blogs.html",
    "title": "Posts",
    "section": "",
    "text": "Low-dimensional representation\n\n\n\n\n\n\n\nBasics\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs there any difference between dimension reductions in \\(2-d\\) and \\(3-d\\)?\n\n\n\n\n\n\n\ntSNE\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow do we set up a Python virtual environment in the R shinyapps.io server?\n\n\n\n\n\n\n\nTechnichal\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\n  \n\n\n\n\nCan the tSNE results be reproducible?\n\n\n\n\n\n\n\ntSNE\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nJayani P.G. Lakshika\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog_2/index.html",
    "href": "posts/blog_2/index.html",
    "title": "How do we set up a Python virtual environment in the R shinyapps.io server?",
    "section": "",
    "text": "Today, I will introduce what are the initial steps that you need to follow when setting up a Python virtual environment in R for the shinyapps.io server.\nNote: You need to have an account in shinyapps.io server. The page can be found in here.\nFirst, you need to create a ‚Äò.Rprofile‚Äô file within the directory of your shiny app. Then, initialize a suitable name for the \\(\\color{red}{\\text{Python virtual environment}}\\).\n\nVIRTUALENV_NAME = \"new_env\"\n\nAfter that, we need to set environment variables in the ‚Äò.Rprofile‚Äô file as follows.\n\nSys.setenv(PYTHON_PATH = 'python3')\n# Installs into default shiny virtualenvs dir\nSys.setenv(VIRTUALENV_NAME = VIRTUALENV_NAME) \nSys.setenv(RETICULATE_PYTHON = paste0('/home/shiny/.virtualenvs/', \n                                      VIRTUALENV_NAME, '/bin/python'))\n\nThe next step is to create the \\(\\color{red}{\\text{Python virtual environment}}\\). To do that, you can write the following code chunks in server.R or your shiny app script.\nIn there, first of all, you need to get the environment variables.\n\nvirtualenv_dir = Sys.getenv(\"VIRTUALENV_NAME\")\npython_path = Sys.getenv(\"PYTHON_PATH\")\n\nNext, create a Python virtual environment by specifying the Python path.\n\nreticulate::virtualenv_create(virtualenv_dir, python = python_path)\n\nThen, you have to install Python dependencies. To do this, you can install the packages by directly specifying them.\n\nreticulate::virtualenv_install(virtualenv_dir, packages = c(\"pandas==1.3.5\")) \n\nIf not, can use requirement.txt which contains all the packages.\n\nreticulate::virtualenv_install(virtualenv_dir, c(\"-r\", \"requirements.txt\"))\n\nFinally, define the Python virtual environment to be used by reticulate.\n\nreticulate::use_virtualenv(virtualenv_dir, required = TRUE)\n\nDone‚Ä¶üëè"
  },
  {
    "objectID": "Learning.html",
    "href": "Learning.html",
    "title": "Introduction to topics",
    "section": "",
    "text": "Local vs global structure\n\n\n\n\n\n\n\nBasics\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "learning/learning_1/index.html",
    "href": "learning/learning_1/index.html",
    "title": "Local vs global structure",
    "section": "",
    "text": "library(Rtsne)\nlibrary(umap)\nlibrary(phateR)\n\nWarning: package 'phateR' was built under R version 4.2.2\n\n\nLoading required package: Matrix\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(langevitour)\nlibrary(reticulate)\nlibrary(ggplot2)\nuse_python(\"~/miniforge3/envs/pcamp_env/bin/python\")\nuse_condaenv(\"pcamp_env\")"
  },
  {
    "objectID": "learning/learning_1/index.html#references",
    "href": "learning/learning_1/index.html#references",
    "title": "Local vs global structure",
    "section": "References",
    "text": "References\nhttps://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"
  },
  {
    "objectID": "posts/blog_3/index.html",
    "href": "posts/blog_3/index.html",
    "title": "Low-dimensional representation",
    "section": "",
    "text": "Today, I will introduce some basic terminologies regarding low-dimensional representation and types of dimensionality reduction techniques and visualize some outcomes of dimensionality reductions of an example dataset.\nLet‚Äôs begin‚Ä¶ü§ì\nThe first question that comes to your mind is,"
  },
  {
    "objectID": "learning/learning_2/index.html",
    "href": "learning/learning_2/index.html",
    "title": "What is meant by low-dimensional representation?",
    "section": "",
    "text": "Today, I will introduce some basic terminologies regarding low-dimensional representation and types of low-dimensional reduction techniques and visualize some outcomes of low-dimensional reductions of an example dataset.\nLet‚Äôs begin‚Ä¶\nThe first question that comes to your mind is, ‚ÄúWhat is meant by low-dimensional representation?‚Äù.\nA low-dimensional representation is a visualization of the dimension reductions on high-dimensional data.\nGot it‚Ä¶\nI know what the next problem you have now is. How do we define a data set as a high-dimensional data set?\nSuppose the number of features (variable observed) is close to or larger than the number of observations (or data points). In that case, the data set is specified as high-dimensional. On the other hand, a low-dimensional data set in which the number of features is less than the number of observations.\nNext,\nWhy do we need a low-dimensional representation?\nThere are various applications in the field of machine learning and deep learning. For example, a low-dimensional data representation can be used as a noise-removal or feature-extraction technique.\nAre there any challenges with high-dimensional data?\nOne of our challenges is that analyzing high-dimensional data requires considering potential problems with more features than observations.\nIn addition, most classical statistical methods are set up for low-dimensional data because low-dimensional data were much more common in the past when data collection was more difficult and time-consuming. However, the development of information technology has allowed large amounts of data to be collected and stored with relative ease, allowing large numbers of features to be collected in recent years.\nAlso, visualizing a large number of features takes a lot of work.\nI hope you all have a basic idea about the terminologies and the challenges with high-dimensional data. The necessity of low-dimension reduction techniques comes as a remedy for the above challenges, especially in the visualization task.\nDimensionality reduction is a common step for data processing. This process is helpful for feature engineering or data visualization. From here onwards, I focus on how the visualization looks in various dimension reduction techniques.\nBefore starting, how many dimensions are defined as low-dimensional reductions?\nGenerally, most dimension reduction techniques allow the reduction of high-dimensional data to two or three dimensions (eg: t-distributed stochastic neighbor embedding (t-SNE)).\nDimension reduction tools for visualization can be categorized as follows.\nLinear dimension reduction techniques\n\nPrincipal Component Analysis (PCA)\nLinear Discriminant Analysis (LDA)\nMultidimensional Scaling (MDS)\n\nNon-linear dimension reduction techniques (Manifold learning)\n\nt-distributed stochastic neighbor embedding (tSNE)\nUniform Manifold Approximation and Projection (UMAP)\nPotential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)\nLarge-scale Dimensionality Reduction Using Triplets (TriMAP)\nPairwise Controlled Manifold Approximation (PaCMAP)\nAuto Encoder (AE)\n\nThe high variance directions reflect the key trends in the data set in a linear dimensionality reduction technique. For example, a new axis is linear and may be non-orthogonal [@paper2]. In contrast, non-linear dimensionality reduction utilizes non-linear kernels to locate key data set trends while preserving the original data‚Äôs local and global structure (neighborhood relation). In terms of preserving the local and global structure, the distance or distance ranks between data points in the original space should be preserved after reducing the dimensions to low-dimensional space.\nYou can find more details about the local and global structure here.\nExample\nA simulated dataset is generated to obtain a \\(2-d\\) plane in \\(4-d\\) with little noise in third and fourth dimensions.\n\n\n\n\n\nCode\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\nlangevitour(df_2)\n\n\n\n\n\nFigure¬†1: Visualization of the simulated dataset to obtain a \\(2-d\\) plane in \\(4-d\\).\n\n\n\nLet‚Äôs visualize PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP and see whether they preserve the \\(2-d\\) plane‚Äôs structure after applying dimension reductions.\nNote: The default parameters are used to perform dimensionality reduction techniques.\n\n\nCode\n## PCA\n\ncalculate_pca <- function(feature_dataset, num_pcs){\n  pcaY_cal <- prcomp(feature_dataset, center = TRUE, scale = TRUE)\n  PCAresults <- data.frame(pcaY_cal$x[, 1:4])\n  summary_pca <- summary(pcaY_cal)\n  var_explained_df <- data.frame(PC= paste0(\"PC\",1:4),\n                               var_explained=(pcaY_cal$sdev[1:4])^2/sum((pcaY_cal$sdev[1:4])^2))\n  return(list(prcomp_out = pcaY_cal,pca_components = PCAresults, summary = summary_pca, var_explained_pca  = var_explained_df))\n}\n\npca_ref_calc <- calculate_pca(df_2,4) \nvar_explained_df <- pca_ref_calc$var_explained_pca\nPCA_df <- pca_ref_calc$pca_components\n\nPCA_df_plot1 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC2)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot2 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot3 <- PCA_df %>%\n  ggplot(aes(x = PC2,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\n## tSNE\nset.seed(100001)\ntSNE_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  Rtsne()\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()  \n\nnames(tSNE_df)[1:2] <- c(\"tSNE1\", \"tSNE2\")\n\ntSNE_df_plot <- tSNE_df %>%\n  ggplot(aes(x = tSNE1,\n             y = tSNE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## UMAP\nset.seed(100002)\nUMAP_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  umap()\n\nUMAP_df <- UMAP_fit$layout %>%\n  as.data.frame()  \n\nnames(UMAP_df)[1:2] <- c(\"UMAP1\", \"UMAP2\")\n\nUMAP_df_plot <- UMAP_df %>%\n  ggplot(aes(x = UMAP1,\n             y = UMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## Phate\n\n# use_python(\"~/miniforge3/envs/pcamp_env/bin/python\")\n# use_condaenv(\"pcamp_env\")\n\n# set.seed(100003)\n# tree_phate_fit <- phate(df_2, 2)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding) %>%\n#   mutate(ID=row_number())\n# \n# names(PHATE_df)[1:2] <- c(\"PHATE1\", \"PHATE2\")\n# write.csv(PHATE_df, paste0(here::here(), \"/learning/learning_2/PHATE_df.csv\"))\n\nPHATE_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PHATE_df.csv\"))\n\nPHATE_df_plot <- PHATE_df %>%\n  ggplot(aes(x = PHATE1,\n             y = PHATE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## TriMAP\n\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# set.seed(100004)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# write.csv(df_TriMAP, paste0(here::here(), \"/learning/learning_2/TriMAP_df.csv\"))\n\n  \n\nTriMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/TriMAP_df.csv\"))\n\nTriMAP_df_plot <- TriMAP_df %>%\n  ggplot(aes(x = TriMAP1,\n             y = TriMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## PaCMAP\n\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# set.seed(100005)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# \n# write.csv(df_PaCMAP, paste0(here::here(), \"/learning/learning_2/PaCMAP_df.csv\"))\n\n\nPaCMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PaCMAP_df.csv\"))\n\nPaCMAP_df_plot <- PaCMAP_df %>%\n  ggplot(aes(x = PaCMAP1,\n             y = PaCMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n (PCA_df_plot1 + PCA_df_plot2 + PCA_df_plot3 ) / ( tSNE_df_plot + UMAP_df_plot + PHATE_df_plot )/( TriMAP_df_plot + PaCMAP_df_plot)\n\n\n\n\n\nFigure¬†2: Visualization by PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP\n\n\n\n\nAs shown in Figure Figure¬†2, you can see the outcomes visualized by some dimensionality reduction techniques.\nIn our next blog, we will see why these dimensionality reduction representations differ from the original structure in high-dimensional space."
  },
  {
    "objectID": "posts/blog_4/index.html",
    "href": "posts/blog_4/index.html",
    "title": "Is there any difference between dimension reductions in \\(2-d\\) and \\(3-d\\)?",
    "section": "",
    "text": "Generally, we reduce the dimensions to 2-d and see the visualization. But you also can try to reduce to \\(3-d\\) as well.\nThe main issue you would in mind is that can it preserve the same structure as in \\(2-d\\).\nLet‚Äôs look an example‚Ä¶\n\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\n\nN <- dim(df_2)[1]\nopt_perplexity <- sqrt(N)\n\ntSNE_fit <- df_2 %>%\n    select(where(is.numeric)) %>%\n    Rtsne(perplexity = opt_perplexity, pca = FALSE, pca_center = FALSE, normalize = FALSE, dims = 2)\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()  %>%\n  mutate(ID=row_number())\n\nnames(tSNE_df)[1:(ncol(tSNE_df)-1)] <- paste0(rep(\"tSNE\",(ncol(tSNE_df)-1)), 1:(ncol(tSNE_df)-1))\n\ntSNE_df_plot <- tSNE_df %>%\n    ggplot(aes(x = tSNE1,\n               y = tSNE2))+\n    geom_point() +\n    coord_equal()\n\ntSNE_df_plot\n\n\n\n\n\ntSNE_fit1 <- df_2 %>%\n    select(where(is.numeric)) %>%\n    Rtsne(perplexity = opt_perplexity, pca = FALSE, pca_center = FALSE, normalize = FALSE, dims = 3)\n\ntSNE_df1 <- tSNE_fit1$Y %>%\n  as.data.frame()  %>%\n  mutate(ID=row_number())\n\nnames(tSNE_df1)[1:(ncol(tSNE_df1)-1)] <- paste0(rep(\"tSNE\",(ncol(tSNE_df1)-1)), 1:(ncol(tSNE_df1)-1))\n\nlangevitour(tSNE_df1 %>% select(-ID))"
  },
  {
    "objectID": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation.",
    "href": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation.",
    "title": "What is meant by low-dimensional representation?",
    "section": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\).",
    "text": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\).\nA low-dimensional representation is a visualization of the dimension reductions on high-dimensional data.\nGot it‚Ä¶üëè\nI know what the next problem you have now is."
  },
  {
    "objectID": "posts/blog_3/index.html#how-do-we-define-a-data-set-as-a-high-dimensional-data-set",
    "href": "posts/blog_3/index.html#how-do-we-define-a-data-set-as-a-high-dimensional-data-set",
    "title": "Low-dimensional representation",
    "section": "How do we define a data set as a high-dimensional data set?",
    "text": "How do we define a data set as a high-dimensional data set?\nSuppose the number of features (variable observed) is close to or larger than the number of observations (or data points). In that case, the data set is specified as \\(\\color{green}{\\text{high-dimensional}}\\). On the other hand, a low-dimensional data set in which the number of features is less than the number of observations.\nNext,"
  },
  {
    "objectID": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation",
    "href": "posts/blog_3/index.html#colorredtextwhat-is-meant-by-low-dimensional-representation",
    "title": "Low-dimensional representation",
    "section": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\)",
    "text": "\\(\\color{red}{\\text{What is meant by low-dimensional representation?}}\\)\nA low-dimensional representation is a visualization of the dimension reductions on high-dimensional data.\nGot it‚Ä¶üëè\nI know what the next problem you have now is."
  },
  {
    "objectID": "posts/blog_3/index.html#why-do-we-need-a-low-dimensional-representation",
    "href": "posts/blog_3/index.html#why-do-we-need-a-low-dimensional-representation",
    "title": "Low-dimensional representation",
    "section": "Why do we need a low-dimensional representation?",
    "text": "Why do we need a low-dimensional representation?\nThere are various applications in the field of machine learning and deep learning. For example, a low-dimensional data representation can be used as a noise-removal or feature-extraction technique."
  },
  {
    "objectID": "posts/blog_3/index.html#are-there-any-challenges-with-high-dimensional-data",
    "href": "posts/blog_3/index.html#are-there-any-challenges-with-high-dimensional-data",
    "title": "Low-dimensional representation",
    "section": "Are there any challenges with high-dimensional data?",
    "text": "Are there any challenges with high-dimensional data?\nOne of our challenges is that analyzing high-dimensional data requires considering potential problems with more features than observations.\nIn addition, most classical statistical methods are set up for low-dimensional data because low-dimensional data were much more common in the past when data collection was more difficult and time-consuming. However, the development of information technology has allowed large amounts of data to be collected and stored with relative ease, allowing large numbers of features to be collected in recent years.\nAlso, visualizing a large number of features takes a lot of work.\nI hope you all have a basic idea about the terminologies and the challenges with high-dimensional data. üëè"
  },
  {
    "objectID": "posts/blog_3/index.html#dimensionality-reduction-techniques",
    "href": "posts/blog_3/index.html#dimensionality-reduction-techniques",
    "title": "Low-dimensional representation",
    "section": "Dimensionality reduction techniques",
    "text": "Dimensionality reduction techniques\nThe necessity of dimensionality reduction techniques comes as a remedy for the above challenges, especially in the visualization task.\nDimensionality reduction is a common step for data processing. This process is helpful for feature engineering or data visualization. From here on wards, I focus on how the visualization looks in various dimension reduction techniques.\nBefore starting, \\(\\color{blue}{\\text{how many dimensions are defined as dimensional reductions in low-dimensional space?}}\\)\nGenerally, most dimension reduction techniques allow the reduction of high-dimensional data to two or three dimensions (eg: t-distributed stochastic neighbor embedding (t-SNE)).\nDimension reduction tools for visualization can be categorized as follows.\n\n\n\n\n\n\n\nLinear dimensionality reduction\ntechniques\nNon-linear dimensionality reduction\ntechniques (Manifold learning)\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\n\nt-distributed stochastic neighbor\nembedding (tSNE)\n\n\n\n\nLinear Discriminant Analysis (LDA)\n\n\nUniform Manifold Approximation and\nProjection (UMAP)\n\n\n\n\nMultidimensional Scaling (MDS)\n\n\nPotential of Heat-diffusion for Affinity-based\nTrajectory Embedding (PHATE)\n\n\n\n\nFactorial Analysis (FA)\n\n\nLarge-scale Dimensionality Reduction\nUsing Triplets (TriMAP)\n\n\n\n\nTruncated Singular Value Decomposition (SVD)\n\n\nPairwise Controlled Manifold\nApproximation (PaCMAP)\n\n\n\n\nGeneralized discriminant analysis (GDA)\n\n\nAuto Encoder (AE)\n\n\n\n\nThe high variance directions reflect the key trends in the data set in a linear dimensionality reduction technique. For example, a new axis is linear and may be non-orthogonal (Parashar et al. 2019). In contrast, non-linear dimensionality reduction utilizes non-linear kernels to locate key data set trends while preserving the original data‚Äôs local and global structure (neighborhood relation). In terms of preserving the local and global structure, the distance or distance ranks between data points in the original space should be preserved after reducing the dimensions to low-dimensional space.\nYou can find more details about the local and global structure here.\n\nExample\nA simulated dataset is generated to obtain a \\(2-d\\) plane in \\(4-d\\) with little noise in third and fourth dimensions.\n\n\n\n\n\nCode\nrandom_num1 <- runif(1, min = 1, max = 10000000)\nset.seed(random_num1)\nu <- runif(1000, min = 10, max = 30)\nv <- runif(1000, min = 10, max = 20)\nx <- u + v - 10\ny <- v - u + 8\nz <- rep(0, 1000) + runif(1000, 0, 1)\nw <- rep(0, 1000) - runif(1000, 0, 1)\n\ndf_2 <- tibble::tibble(x1 = x, x2 = y, x3 = z, x4 = w) \n\nlangevitour(df_2)\n\n\n\n\n\nFigure¬†1: Visualization of the simulated dataset to obtain a \\(2-d\\) plane in \\(4-d\\).\n\n\n\nLet‚Äôs visualize PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP and see whether they preserve the \\(2-d\\) plane‚Äôs structure after applying dimension reductions.\nNote: The default parameters are used to perform dimensionality reduction techniques.\n\n\nCode\n## PCA\n\ncalculate_pca <- function(feature_dataset, num_pcs){\n  pcaY_cal <- prcomp(feature_dataset, center = TRUE, scale = TRUE)\n  PCAresults <- data.frame(pcaY_cal$x[, 1:4])\n  summary_pca <- summary(pcaY_cal)\n  var_explained_df <- data.frame(PC= paste0(\"PC\",1:4),\n                               var_explained=(pcaY_cal$sdev[1:4])^2/sum((pcaY_cal$sdev[1:4])^2))\n  return(list(prcomp_out = pcaY_cal,pca_components = PCAresults, summary = summary_pca, var_explained_pca  = var_explained_df))\n}\n\npca_ref_calc <- calculate_pca(df_2,4) \nvar_explained_df <- pca_ref_calc$var_explained_pca\nPCA_df <- pca_ref_calc$pca_components\n\nPCA_df_plot1 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC2)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot2 <- PCA_df %>%\n  ggplot(aes(x = PC1,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\nPCA_df_plot3 <- PCA_df %>%\n  ggplot(aes(x = PC2,\n             y = PC3)) +\n  geom_point() +\n  coord_fixed()\n\n## tSNE\nset.seed(100001)\ntSNE_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  Rtsne()\n\ntSNE_df <- tSNE_fit$Y %>%\n  as.data.frame()  \n\nnames(tSNE_df)[1:2] <- c(\"tSNE1\", \"tSNE2\")\n\ntSNE_df_plot <- tSNE_df %>%\n  ggplot(aes(x = tSNE1,\n             y = tSNE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## UMAP\nset.seed(100002)\nUMAP_fit <- df_2 %>%\n  select(where(is.numeric)) %>%\n  umap()\n\nUMAP_df <- UMAP_fit$layout %>%\n  as.data.frame()  \n\nnames(UMAP_df)[1:2] <- c(\"UMAP1\", \"UMAP2\")\n\nUMAP_df_plot <- UMAP_df %>%\n  ggplot(aes(x = UMAP1,\n             y = UMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## Phate\n\n# use_python(\"~/miniforge3/envs/pcamp_env/bin/python\")\n# use_condaenv(\"pcamp_env\")\n\n# set.seed(100003)\n# tree_phate_fit <- phate(df_2, 2)\n# \n# PHATE_df <- as.data.frame(tree_phate_fit$embedding) %>%\n#   mutate(ID=row_number())\n# \n# names(PHATE_df)[1:2] <- c(\"PHATE1\", \"PHATE2\")\n# write.csv(PHATE_df, paste0(here::here(), \"/learning/learning_2/PHATE_df.csv\"))\n\nPHATE_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PHATE_df.csv\"))\n\nPHATE_df_plot <- PHATE_df %>%\n  ggplot(aes(x = PHATE1,\n             y = PHATE2)) +\n  geom_point() +\n  coord_fixed()\n\n\n## TriMAP\n\n# source_python(\"Fit_TriMAP_code.py\")\n# \n# n_inliers_n <- as.integer(12)\n# n_outliers_n <- as.integer(4)\n# n_random_n <- as.integer(3)\n# \n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# \n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_TriMAP_values.csv\")\n# \n# set.seed(100004)\n# \n# Fit_TriMAP(as.integer(2), n_inliers_n, n_outliers_n, n_random_n, path, path2)\n# \n# df_TriMAP <- read.csv(path2)\n# write.csv(df_TriMAP, paste0(here::here(), \"/learning/learning_2/TriMAP_df.csv\"))\n\n  \n\nTriMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/TriMAP_df.csv\"))\n\nTriMAP_df_plot <- TriMAP_df %>%\n  ggplot(aes(x = TriMAP1,\n             y = TriMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n## PaCMAP\n\n# source_python(\"Fit_PacMAP_code.py\")\n# \n# knn_n <- as.integer(10)\n# init_n <- \"random\"\n# MN_ratio_n <- 0.5\n# FP_ratio_n <- 2.0\n# data_pca <- df_2 %>%\n#   select(where(is.numeric))\n# \n# tem_dir <- tempdir()\n# \n# write.csv(data_pca, file.path(tem_dir, \"df_2_without_class.csv\"), row.names = FALSE,\n#             quote = TRUE)\n# set.seed(100005)\n# path <- file.path(tem_dir, \"df_2_without_class.csv\")\n# path2 <- file.path(tem_dir, \"dataset_3_PaCMAP_values.csv\")\n# Fit_PaCMAP(as.integer(2), knn_n, init_n, MN_ratio_n, FP_ratio_n, path, path2)\n# \n# df_PaCMAP <- read.csv(path2)\n# \n# write.csv(df_PaCMAP, paste0(here::here(), \"/learning/learning_2/PaCMAP_df.csv\"))\n\n\nPaCMAP_df <- read_csv(paste0(here::here(), \"/posts/blog_3/PaCMAP_df.csv\"))\n\nPaCMAP_df_plot <- PaCMAP_df %>%\n  ggplot(aes(x = PaCMAP1,\n             y = PaCMAP2)) +\n  geom_point() +\n  coord_fixed()\n\n (PCA_df_plot1 + PCA_df_plot2 + PCA_df_plot3 ) / ( tSNE_df_plot + UMAP_df_plot + PHATE_df_plot )/( TriMAP_df_plot + PaCMAP_df_plot)\n\n\n\n\n\nFigure¬†2: Visualization by PCA, t-SNE, UMAP, PHATE, TriMAP, and PaCMAP\n\n\n\n\nAs shown in Figure¬†2, you can see the outcomes visualized by some dimensionality reduction techniques.\nIn our next blog, we will see how and why these dimensionality reduction representations differ from the original structure in high-dimensional space."
  }
]